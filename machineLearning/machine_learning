1.supervised learning : 
    (1)give the input and output, or question and right answer, produce the algorithm
    (2)the main form: regression and classification.
    (3)regression: the answer haven't specific number range,like house price releate with size
    (4)classification: the answer have specific categories, like tumor benight and malignant releate with size and patient age.
2.unsupervised learning:
    (1)give dataset,don't give right answer,machine found the similar data together in dataset, and produce the algorith.Like the customer group class
    (2)cluster algorithm
3.standard notation
    supervised learning: a part of (x,y) data refer single data example. x and y have superscript (i) , the (i) refer each row data example.
4.linear regresion 
    f(x)=wx + b
    the value of expression is called y-hat, it is predict value, not the truth value.
5.gradient descent
    (1)the best model is the y-hat which most close truth y. use J(w,b) representation.
    J(w,b) = squaries erro (y-hat^i minus y^i),and the sum of all examples.
    as small as possible for J(w,b),the minumize of J, the w ,b is best value.
    (2)w & b ,are litera,simultaneous update,expression is
    w = w - a* c/cz*J(w,b)
    b = b - a* c/cz*J(w,b)
6.multiple feature
    called vectorization, multiple linear regression,like:
        f_w,b(x)=w_1*x_1 + w_2*x_2 + w_3*x_3 + …… +w_n*x_n + b
        define :
        x_1^1 reference in sample,the fisrt row and fist feature value
        x_1^2 reference in sample,the second row and fist feature value
        x_2^3 reference in sample,the third row and second feature value
    simply expression,a really simplify trick，called vectorization
        f_w-arrow,b(x-arrow)=w-arrow * x-arrow +b
7.vectorization
    use vectorization code in python
    import NumPy as np
    f = np.dot(w,x)
8.feature scale
    let each feature comparable by scale,like 300<x_1<2000 , by divde 2000 to scaling 0.15<x_1<1
    two common method:  mean normalization and Z-score normalization
9.judge cost function J_w,b(x) is if converging
    by learning curv, by the increase of number of iterations, the J_wb(x) will more and more less until flat, declare the it is converging. other wise,the learning rate is too large or the code has bug.
    the curv is quickly decrease, so the learning rate value is appropriate.
    the curv is slowly decrease, the learning rate value is too small.
    the curv is increase and decrease or continuous increase, the learing rate is too large.
10.classification prolem
    use logistic regression to solve, do not use linear regression which not fit to solve .
    draw decision boundries to classification result, the equation is f_w-hat,b(x-hat) = g(z) = 1/(1+e^(-z))
11.logistic regression
    use loss function express 
12.overfitting
    also say large variance ,contract underfitting or high bias. just well is called generalization.
13.addressing overfitting
    (1)collect more data examples: so that train a new model
    (2)reduce the features: pick out the most useful features to train , eliminate other features
    (3)regulization: minimize the parameter of high index features, reduce the impact which features